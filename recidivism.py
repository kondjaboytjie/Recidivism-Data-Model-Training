# -*- coding: utf-8 -*-
"""Recidivism.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G5HTljAxO-CRvyj4htYYNngRlFJwsB48
"""

#importing packages
import pandas as pd
import numpy as np
import sklearn
from google.colab import drive

drive = drive.mount('/content/drive')

"""Data Proccessing"""

#loading data
file_path = f'/content/drive/My Drive/Recidivism_Main_File.csv'

df_data = pd.read_csv(file_path)

#showing data
df_data

#checks for dataset variables
df_data.info()

print(df_data.isnull().sum())

# Display the column names
print(df_data.columns)

import pandas as pd

# Filling missing values
df_data['Release Type'].fillna(df_data['Release Type'].mode()[0], inplace=True)
df_data['Main Supervising District'].fillna(df_data['Main Supervising District'].mode()[0], inplace=True)
df_data['Release type: Paroled to Detainder united'].fillna(df_data['Release type: Paroled to Detainder united'].mode()[0], inplace=True)
df_data['Age At Release '].fillna(df_data['Age At Release '].mode()[0], inplace=True)

# Display the updated DataFrame
print(df_data)

import pandas as pd

# Diving the dataset into depented and indepent variables
X = df_data.drop('Recidivism - Return to Prison numeric', axis=1)
y = df_data['Recidivism - Return to Prison numeric']


# Print the shape of the independent and dependent variables
print("Independent variable shape:", y.shape)
print("Dependent variables shape:", X.shape)

# Showing ethinicity
import pandas as pd


race_ethnicity_column = df_data['Race - Ethnicity']

race_ethnicity_counts = race_ethnicity_column.value_counts()

for race_ethnicity, count in race_ethnicity_counts.items():
    print(race_ethnicity, count)

#counting total records
count = len(df_data)
print("Count:", count)

#counting total records where Recidivism - Return to Prison is Yes
count = df_data[df_data['Recidivism - Return to Prison numeric'] >= 1].shape[0]

print("Recidivism Count:", count)

#counting total records where Recidivism - Return to Prison is Yes
count = df_data[df_data['Recidivism - Return to Prison numeric'] == 0].shape[0]

print("None Recidivism Count:", count)

#removing the Main Supervising District
df_data = df_data.drop('Main Supervising District', axis=1)

#checks for dataset variables
df_data.info()

#checking the years appearing
# Get the unique years
unique_years = df_data['Recidivism Reporting Year'].unique()

# Print the unique years
for year in unique_years:
    print(year)

"""End of Data proccessing"""

import matplotlib.pyplot as plt

# Get the counts for each year
counts_0 = df_data[df_data['Recidivism - Return to Prison numeric'] == 0].groupby('Recidivism Reporting Year').size()
counts_1 = df_data[df_data['Recidivism - Return to Prison numeric'] >= 1].groupby('Recidivism Reporting Year').size()

# Create a bar chart
plt.bar(counts_0.index, counts_0.values, label='No Recidivism')
plt.bar(counts_1.index, counts_1.values, label='Recidivism')

# Set the x-axis label
plt.xlabel('Recidivism Reporting Year')

# Set the y-axis label
plt.ylabel('Count')

# Title the graph
plt.title('Recidivism Count by Year')

# Add labels to the bars
for i, count in enumerate(counts_0.values):
    plt.text(counts_0.index[i], count, str(count), ha='center', va='bottom')
for i, count in enumerate(counts_1.values):
    plt.text(counts_1.index[i], count, str(count), ha='center', va='bottom')

# Add legend
plt.legend()

# Show the graph
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

release_types = df_data['Release Type'].value_counts()

plt.bar(release_types.index, release_types.values)
plt.xticks(rotation=90)
plt.ylabel('Count')
plt.title('Number of Releases by Type')

for i, v in enumerate(release_types.values):
  plt.text(i, v, v, ha='center', va='bottom')

plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Filter the relevant columns
df_subset = df_data[['Release Type', 'Recidivism - Return to Prison numeric']]

# Drop rows with missing values in either column
df_subset = df_subset.dropna()

# Set the figure size
plt.figure(figsize=(10, 6))

# Create a count plot to visualize the relationship
ax = sns.countplot(data=df_subset, x='Release Type', hue='Recidivism - Return to Prison numeric', dodge=True)

# Rotate x-axis labels for better readability
plt.xticks(rotation=90)

# Set the y-axis label
plt.ylabel('Count')

# Title the graph
plt.title('Relationship between Release Type and Recidivism')

# Add amount labels to the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')

# Show the graph
plt.show()

"""# **Model Training**"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Create a new DataFrame for encoded features
df_encoded = pd.get_dummies(df_data)

# Extract the target variable
y = df_encoded['Recidivism - Return to Prison numeric']

# Drop the target variable from the encoded DataFrame
X = df_encoded.drop('Recidivism - Return to Prison numeric', axis=1)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Calculate the correlation matrix
correlation_matrix = df_encoded.corr()

# Extract the correlation values with the target variable
correlation_with_target = correlation_matrix['Recidivism - Return to Prison numeric']

# Sort the correlation values in descending order
correlation_sorted = correlation_with_target.abs().sort_values(ascending=False)

# Print the correlation values
print(correlation_sorted)

"""**Based on the provided correlation values, the features that have relatively higher correlations with the target variable are:**

**Release Type_Parole**
**Release type: Paroled to Detainder united_Parole**
**Part of Target Population_No**
**Part of Target Population_Yes**

**Naive Bayes:**
"""

from sklearn.naive_bayes import GaussianNB

# Train Na誰ve Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions on the test data
nb_predictions = nb_model.predict(X_test)

"""**Support Vector Machine (SVM):**"""

from sklearn.svm import SVC

# Train SVM model
svm_model = SVC()
svm_model.fit(X_train, y_train)

# Make predictions on the test data
svm_predictions = svm_model.predict(X_test)

"""**Random Forest:**"""

from sklearn.ensemble import RandomForestClassifier

# Train Random Forest model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Make predictions on the test data
rf_predictions = rf_model.predict(X_test)

"""**Artificial Neural Networks (Multi-Layer Perceptron):**"""

from sklearn.neural_network import MLPClassifier

# Train ANN model
ann_model = MLPClassifier()
ann_model.fit(X_train, y_train)

# Make predictions on the test data
ann_predictions = ann_model.predict(X_test)

"""**Decision Tree:**"""

from sklearn.tree import DecisionTreeClassifier

# Train Decision Tree model
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

# Make predictions on the test data
dt_predictions = dt_model.predict(X_test)

"""# **Model Evaluation**

**Naive Bayes**
"""

# Import necessary libraries
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Evaluate Na誰ve Bayes model
nb_accuracy = accuracy_score(y_test, nb_predictions)
nb_precision = precision_score(y_test, nb_predictions)
nb_recall = recall_score(y_test, nb_predictions)
nb_f1 = f1_score(y_test, nb_predictions)
nb_roc_auc = roc_auc_score(y_test, nb_predictions)
nb_confusion_matrix = confusion_matrix(y_test, nb_predictions)

# Print evaluation metrics
print("Na誰ve Bayes Evaluation:")
print("Accuracy:", nb_accuracy)
print("Precision:", nb_precision)
print("Recall:", nb_recall)
print("F1-Score:", nb_f1)
print("ROC AUC:", nb_roc_auc)
print("Confusion Matrix:\n", nb_confusion_matrix)

"""**Support Vector Machine (SVM):**"""

# Evaluate SVM model
svm_accuracy = accuracy_score(y_test, svm_predictions)
svm_precision = precision_score(y_test, svm_predictions)
svm_recall = recall_score(y_test, svm_predictions)
svm_f1 = f1_score(y_test, svm_predictions)
svm_roc_auc = roc_auc_score(y_test, svm_predictions)
svm_confusion_matrix = confusion_matrix(y_test, svm_predictions)

# Print evaluation metrics
print("Na誰ve Bayes Evaluation:")
print("Accuracy:", svm_accuracy)
print("Precision:", svm_precision)
print("Recall:", svm_recall)
print("F1-Score:", svm_f1)
print("ROC AUC:", svm_roc_auc)
print("Confusion Matrix:\n", svm_confusion_matrix)

"""**Random Forest:**"""

# Evaluate Random Forest model
rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_precision = precision_score(y_test, rf_predictions)
rf_recall = recall_score(y_test, rf_predictions)
rf_f1 = f1_score(y_test, rf_predictions)
rf_roc_auc = roc_auc_score(y_test, rf_predictions)
rf_confusion_matrix = confusion_matrix(y_test, rf_predictions)

# Print evaluation metrics
print("Random Forest Evaluation:")
print("Accuracy:", rf_accuracy)
print("Precision:", rf_precision)
print("Recall:", rf_recall)
print("F1-Score:", rf_f1)
print("ROC AUC:", rf_roc_auc)
print("Confusion Matrix:\n", rf_confusion_matrix)

"""**Artificial Neural Networks (ANN):**"""

# Evaluate ANN model
ann_accuracy = accuracy_score(y_test, ann_predictions)
ann_precision = precision_score(y_test, ann_predictions)
ann_recall = recall_score(y_test, ann_predictions)
ann_f1 = f1_score(y_test, ann_predictions)
ann_roc_auc = roc_auc_score(y_test, ann_predictions)
ann_confusion_matrix = confusion_matrix(y_test, ann_predictions)

# Print evaluation metrics
print("Artificial Neural Networks (ANN) Evaluation:")
print("Accuracy:", ann_accuracy)
print("Precision:", ann_precision)
print("Recall:", ann_recall)
print("F1-Score:", ann_f1)
print("ROC AUC:", ann_roc_auc)
print("Confusion Matrix:\n", ann_confusion_matrix)

"""**Decision Tree:**"""

# Evaluate Decision Tree model
dt_accuracy = accuracy_score(y_test, dt_predictions)
dt_precision = precision_score(y_test, dt_predictions)
dt_recall = recall_score(y_test, dt_predictions)
dt_f1 = f1_score(y_test, dt_predictions)
dt_roc_auc = roc_auc_score(y_test, dt_predictions)
dt_confusion_matrix = confusion_matrix(y_test, dt_predictions)

# Print evaluation metrics
print("Decision Tree Evaluation:")
print("Accuracy:", dt_accuracy)
print("Precision:", dt_precision)
print("Recall:", dt_recall)
print("F1-Score:", dt_f1)
print("ROC AUC:", dt_roc_auc)
print("Confusion Matrix:\n", dt_confusion_matrix)

"""**Grouped results**"""

from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, mean_squared_error

# Train Na誰ve Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_predictions = nb_model.predict(X_test)

# Train Support Vector Machine model
svm_model = SVC()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# Train Random Forest model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Train Artificial Neural Networks model
ann_model = MLPClassifier()
ann_model.fit(X_train, y_train)
ann_predictions = ann_model.predict(X_test)

# Train Decision Tree model
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)

# Evaluate models
models = {
    'Na誰ve Bayes': nb_predictions,
    'Support Vector Machine': svm_predictions,
    'Random Forest': rf_predictions,
    'Artificial Neural Networks': ann_predictions,
    'Decision Tree': dt_predictions
}

# Print evaluation metrics for each model
for model_name, predictions in models.items():
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    roc_auc = roc_auc_score(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)
    confusion = confusion_matrix(y_test, predictions)

    print(f"Model: {model_name}")
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1-Score:", f1)
    print("ROC AUC:", roc_auc)
    print("Mean Squared Error:", mse)
    print("Confusion Matrix:\n", confusion)
    print("-------------------------------------------")

import matplotlib.pyplot as plt

# Evaluation metrics
model_names = ['Na誰ve Bayes', 'Support Vector Machine', 'Random Forest', 'Artificial Neural Networks', 'Decision Tree']
accuracies = [accuracy * 100, svm_accuracy * 100, rf_accuracy * 100, ann_accuracy * 100, dt_accuracy * 100]
precisions = [precision * 100, svm_precision * 100, rf_precision * 100, ann_precision * 100, dt_precision * 100]
recalls = [recall * 100, svm_recall * 100, rf_recall * 100, ann_recall * 100, dt_recall * 100]
f1_scores = [f1 * 100, svm_f1 * 100, rf_f1 * 100, ann_f1 * 100, dt_f1 * 100]
roc_aucs = [roc_auc * 100, svm_roc_auc * 100, rf_roc_auc * 100, ann_roc_auc * 100, dt_roc_auc * 100]

# Create a bar plot
plt.figure(figsize=(10, 6))
x = range(len(model_names))
width = 0.15

plt.bar(x, accuracies, width, label='Accuracy')
plt.bar([i + width for i in x], precisions, width, label='Precision')
plt.bar([i + 2 * width for i in x], recalls, width, label='Recall')
plt.bar([i + 3 * width for i in x], f1_scores, width, label='F1-Score')
plt.bar([i + 4 * width for i in x], roc_aucs, width, label='ROC AUC')

plt.xlabel('Models')
plt.ylabel('Scores (%)')
plt.title('Model Performance')
plt.xticks([i + 2 * width for i in x], model_names)
plt.legend()

# Add labels to the bars
for i, v in enumerate(accuracies):
    plt.text(i - 0.2, v + 1, f'{v:.2f}', color='black', fontweight='bold')
for i, v in enumerate(precisions):
    plt.text(i + 0.02, v + 1, f'{v:.2f}', color='black', fontweight='bold')
for i, v in enumerate(recalls):
    plt.text(i + 0.22, v + 1, f'{v:.2f}', color='black', fontweight='bold')
for i, v in enumerate(f1_scores):
    plt.text(i + 0.42, v + 1, f'{v:.2f}', color='black', fontweight='bold')
for i, v in enumerate(roc_aucs):
    plt.text(i + 0.62, v + 1, f'{v:.2f}', color='black', fontweight='bold')

plt.tight_layout()
plt.show()

"""# **Hyper paremeter Tuning**




"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Create the SVM model
svm_model = SVC()

# Create GridSearchCV instance
grid_search = GridSearchCV(svm_model, param_grid, scoring='accuracy', cv=5)

# Fit the training data to perform grid search
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train SVM model with best parameters
best_svm_model = SVC(**best_params)
best_svm_model.fit(X_train, y_train)

# Make predictions using the tuned SVM model
svm_predictions_tuned = best_svm_model.predict(X_test)

for i in range(len(rf_predictions)):
    print("Record", i+1, "Prediction:", rf_predictions[i])

# Preprocess the input data
input_data = pd.DataFrame({
    'Race - Ethnicity': ['White - Hispanic'],
    'Age At Release': ['Under 35'],
    'Convicting Offense Classification': ['D Felony'],
    'Convicting Offense Type': ['Violent'],
    'Convicting Offense Subtype': ['Sex'],
    'Release Type': ['Special Sentence'],
    'Part of Target Population': ['Yes']
})

# Apply the same preprocessing steps as used in training
input_data_encoded = pd.get_dummies(input_data, drop_first=True) # Encode categorical features
input_data_encoded = input_data_encoded.reindex(columns=X_train.columns, fill_value=0)  # Reindex to match training data columns



# Print the prediction
print("Naive Bayes's offenders likelihood of recidivism is: ", nb_model.predict(input_data_encoded))
print("Support Vector Machine's offenders likelihood of recidivism is: ", svm_model.predict(input_data_encoded))
print("Random forest's offenders likelihood of recidivism is is:", rf_model.predict(input_data_encoded))
print("Artificial Neural Network's offenders likelihood of recidivism is:", ann_model.predict(input_data_encoded))
print("Decision Tree's offenders likelihood of recidivism is:", dt_model.predict(input_data_encoded))